{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2,1)\n",
    "y = np.ones((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.34743453],\n",
       "        [0.5971142 ]]), array([[1.]]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Entity\n",
    "class ReLU: \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        return np.where(Z>0, Z, np.zeros_like(Z))\n",
    "        \n",
    "    def __call__(self, Z):\n",
    "        return self.forward(Z)\n",
    "    \n",
    "    def grad(self, Z):\n",
    "        return {\"i\": self.grad_i(Z)}\n",
    "        \n",
    "    def grad_i(self, Z):\n",
    "        return np.diag(np.where(Z>0, np.ones_like(Z), np.zeros_like(Z)).reshape(-1))\n",
    "        \n",
    "    def update(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "class Sigmoid:\n",
    "    def  __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    def __call__(self, Z):\n",
    "        return self.forward(Z)\n",
    "    \n",
    "    def grad(self, Z):\n",
    "        return {\"i\": self.grad_i(Z)}\n",
    "    \n",
    "    def grad_i(self, Z):\n",
    "        y = self(Z)\n",
    "        return np.diag((y*(1-y)).reshape(-1))\n",
    "    \n",
    "    def update(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer entity\n",
    "class Dense: \n",
    "    \n",
    "    def __init__(self, no_of_neurons, input_size):\n",
    "        self.no_of_neurons = no_of_neurons\n",
    "        \n",
    "        self.W = np.random.randn(input_size, no_of_neurons)\n",
    "        self.b = np.random.randn(no_of_neurons, 1)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def grad(self, X):\n",
    "        return {\"i\": self.grad_i(X), \"b\": self.grad_b(X), \"w\": self.grad_w(X)}\n",
    "    \n",
    "    def grad_i(self, X):\n",
    "        return self.W.T\n",
    "    \n",
    "    def grad_b(self, X):\n",
    "        return np.identity(self.no_of_neurons)\n",
    "    \n",
    "    def grad_w(self, X):\n",
    "        m, n = self.W.shape\n",
    "        return (np.repeat(np.eye(n), repeats=m, axis=0)*np.repeat(np.expand_dims(X, 0), repeats=n, axis=0).reshape(m*n, 1)).reshape(n,m,n)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return np.matmul(self.W.T, X) + self.b        \n",
    "    \n",
    "    def update(self, grad, func):\n",
    "        \n",
    "        self.W = func(self.W, grad[\"w\"])\n",
    "        self.b = func(self.b, grad[\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model entity \n",
    "class Sequential:\n",
    "    \n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def forward(self, X, with_grad=False):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            if with_grad:\n",
    "                grads.append(layer.grad(X))\n",
    "            X = layer.forward(X)\n",
    "        return (X, grads) if with_grad else X\n",
    "    \n",
    "    def backward(self, grads, loss_grad):\n",
    "        loss_grads = []\n",
    "        grads.reverse()\n",
    "        \n",
    "        for grad in grads:\n",
    "            g = {}\n",
    "            if grad.get(\"w\", None) is not None: \n",
    "                g[\"w\"] = np.einsum(\"ij,jkl->ikl\", loss_grad, grad[\"w\"])[0]\n",
    "            if grad.get(\"b\", None) is not None:\n",
    "                g[\"b\"] = np.einsum(\"ij,jk->ik\", loss_grad, grad[\"b\"]).T\n",
    "                \n",
    "            g[\"i\"] = np.matmul(loss_grad, grad[\"i\"])\n",
    "            loss_grads.append(g)\n",
    "            loss_grad = g[\"i\"]\n",
    "        \n",
    "        loss_grads.reverse()\n",
    "        \n",
    "        return loss_grads        \n",
    "    \n",
    "    def step(self, X, y_true, loss, optim):\n",
    "        y_pred, grads = self.forward(X, with_grad=True)\n",
    "        l = loss(y_pred=y_pred, y_true=y_true)\n",
    "        loss_grads = self.backward(grads, loss.grad_i(y_pred=y_pred, y_true=y_true))\n",
    "        for (layer, grad) in zip(self.layers, loss_grads):\n",
    "            optim.step(layer, grad)\n",
    "        \n",
    "    def fit(self, X, y_true, loss, optim, n_epochs):\n",
    "        for i in range(n_epochs):\n",
    "            self.step(X, y_true, loss, optim)\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optim = self.optimizer()\n",
    "   \n",
    "    def optimizer(self):\n",
    "        raise NotImplementedError(\"optimizer method not defined\")\n",
    "        \n",
    "    def step(self, layer, grad):\n",
    "        layer.update(grad, self.optim)\n",
    "    \n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        super().__init__()\n",
    "    \n",
    "    def optimizer(self):\n",
    "        def func(w, dw):\n",
    "            assert w.shape == dw.shape, f\"shape mismatch {w.shape} and {dw.shape}\"\n",
    "            return w - self.lr*dw\n",
    "        return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss entity\n",
    "class BinaryCrossEntropy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        return self.forward(y_pred, y_true)\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return np.where(y_true == 0, -np.log(1-y_pred), -np.log(y_pred))\n",
    "    \n",
    "    def grad_i(self, y_pred, y_true):\n",
    "        return np.where(y_true == 0, -1/(1-y_pred), -1/y_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Dense(2, 2)\n",
    "act1 = ReLU()\n",
    "layer2 = Dense(2, 2)\n",
    "act2 = ReLU()\n",
    "layer3 = Dense(1, 2)\n",
    "act3 = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(layers=[layer1, act1, layer2, act2, layer3, act3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = BinaryCrossEntropy()\n",
    "optim = SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, grads = model.forward(x, with_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "predicted: 0.8481892913872623, true: 1.0, loss: 0.16465144712478388\r",
      "predicted: 0.8483846659477873, true: 1.0, loss: 0.16442113053817833\r",
      "predicted: 0.8485795829151099, true: 1.0, loss: 0.16419140623100986\r",
      "predicted: 0.8487740437752757, true: 1.0, loss: 0.1639622720565412\r",
      "predicted: 0.848968050008384, true: 1.0, loss: 0.16373372587786872\r",
      "predicted: 0.8491616030886169, true: 1.0, loss: 0.16350576556786764\r",
      "predicted: 0.8493547044842653, true: 1.0, loss: 0.16327838900913877\r",
      "predicted: 0.8495473556577557, true: 1.0, loss: 0.1630515940939568\r",
      "predicted: 0.8497395580656771, true: 1.0, loss: 0.16282537872421693\r",
      "predicted: 0.8499313131588079, true: 1.0, loss: 0.16259974081138248\r",
      "predicted: 0.8501226223821413, true: 1.0, loss: 0.16237467827643404\r",
      "predicted: 0.8503134871749132, true: 1.0, loss: 0.1621501890498166\r",
      "predicted: 0.8505039089706267, true: 1.0, loss: 0.16192627107138965\r",
      "predicted: 0.8506938891970794, true: 1.0, loss: 0.1617029222903753\r",
      "predicted: 0.8508834292763894, true: 1.0, loss: 0.16148014066530766\r",
      "predicted: 0.8510725306250195, true: 1.0, loss: 0.1612579241639838\r",
      "predicted: 0.8512611946538052, true: 1.0, loss: 0.16103627076341193\r",
      "predicted: 0.8514494227679783, true: 1.0, loss: 0.16081517844976342\r",
      "predicted: 0.8516372163671938, true: 1.0, loss: 0.16059464521832198\r",
      "predicted: 0.8518245768455541, true: 1.0, loss: 0.1603746690734359\r",
      "predicted: 0.8520115055916349, true: 1.0, loss: 0.16015524802846856\r",
      "predicted: 0.85219800398851, true: 1.0, loss: 0.15993638010574987\r",
      "predicted: 0.8523840734137763, true: 1.0, loss: 0.15971806333652872\r",
      "predicted: 0.852569715239579, true: 1.0, loss: 0.15950029576092425\r",
      "predicted: 0.8527549308326354, true: 1.0, loss: 0.15928307542787942\r",
      "predicted: 0.8529397215542605, true: 1.0, loss: 0.1590664003951127\r",
      "predicted: 0.8531240887603911, true: 1.0, loss: 0.158850268729071\r",
      "predicted: 0.8533080338016099, true: 1.0, loss: 0.1586346785048843\r",
      "predicted: 0.8534915580231699, true: 1.0, loss: 0.15841962780631805\r",
      "predicted: 0.8536746627650189, true: 1.0, loss: 0.15820511472572696\r",
      "predicted: 0.8538573493618232, true: 1.0, loss: 0.15799113736400966\r",
      "predicted: 0.854039619142991, true: 1.0, loss: 0.15777769383056361\r",
      "predicted: 0.8542214734326974, true: 1.0, loss: 0.15756478224323806\r",
      "predicted: 0.8544029135499067, true: 1.0, loss: 0.1573524007282911\r",
      "predicted: 0.8545839408083972, true: 1.0, loss: 0.15714054742034334\r",
      "predicted: 0.8547645565167833, true: 1.0, loss: 0.15692922046233457\r",
      "predicted: 0.8549447619785402, true: 1.0, loss: 0.15671841800547856\r",
      "predicted: 0.8551245584920262, true: 1.0, loss: 0.15650813820921974\r",
      "predicted: 0.8553039473505059, true: 1.0, loss: 0.1562983792411896\r",
      "predicted: 0.8554829298421737, true: 1.0, loss: 0.15608913927716261\r",
      "predicted: 0.8556615072501759, true: 1.0, loss: 0.15588041650101409\r",
      "predicted: 0.8558396808526342, true: 1.0, loss: 0.1556722091046763\r",
      "predicted: 0.8560174519226675, true: 1.0, loss: 0.15546451528809688\r",
      "predicted: 0.856194821728416, true: 1.0, loss: 0.15525733325919494\r",
      "predicted: 0.8563717915330614, true: 1.0, loss: 0.1550506612338209\r",
      "predicted: 0.8565483625948512, true: 1.0, loss: 0.1548444974357131\r",
      "predicted: 0.8567245361671199, true: 1.0, loss: 0.15463884009645693\r",
      "predicted: 0.8569003134983111, true: 1.0, loss: 0.15443368745544328\r",
      "predicted: 0.8570756958320002, true: 1.0, loss: 0.1542290377598274\r",
      "predicted: 0.8572506844069155, true: 1.0, loss: 0.1540248892644881\r",
      "predicted: 0.8574252804569603, true: 1.0, loss: 0.1538212402319876\r",
      "predicted: 0.8575994852112351, true: 1.0, loss: 0.15361808893252982\r",
      "predicted: 0.857773299894058, true: 1.0, loss: 0.1534154336439219\r",
      "predicted: 0.8579467257249872, true: 1.0, loss: 0.15321327265153362\r",
      "predicted: 0.858119763918842, true: 1.0, loss: 0.15301160424825722\r",
      "predicted: 0.8582924156857241, true: 1.0, loss: 0.15281042673446896\r",
      "predicted: 0.8584646822310386, true: 1.0, loss: 0.1526097384179894\r",
      "predicted: 0.8586365647555153, true: 1.0, loss: 0.15240953761404438\r",
      "predicted: 0.8588080644552297, true: 1.0, loss: 0.1522098226452267\r",
      "predicted: 0.8589791825216234, true: 1.0, loss: 0.15201059184145765\r",
      "predicted: 0.8591499201415254, true: 1.0, loss: 0.1518118435399484\r",
      "predicted: 0.8593202784971725, true: 1.0, loss: 0.15161357608516196\r",
      "predicted: 0.8594902587662296, true: 1.0, loss: 0.15141578782877635\r",
      "predicted: 0.8596598621218109, true: 1.0, loss: 0.1512184771296457\r",
      "predicted: 0.8598290897324993, true: 1.0, loss: 0.15102164235376386\r",
      "predicted: 0.8599979427623673, true: 1.0, loss: 0.150825281874227\r",
      "predicted: 0.860166422370997, true: 1.0, loss: 0.15062939407119671\r",
      "predicted: 0.8603345297135003, true: 1.0, loss: 0.15043397733186323\r",
      "predicted: 0.8605022659405384, true: 1.0, loss: 0.15023903005040937\r",
      "predicted: 0.860669632198342, true: 1.0, loss: 0.1500445506279738\r",
      "predicted: 0.8608366296287312, true: 1.0, loss: 0.1498505374726156\r",
      "predicted: 0.8610032593691349, true: 1.0, loss: 0.14965698899927773\r",
      "predicted: 0.8611695225526106, true: 1.0, loss: 0.14946390362975204\r",
      "predicted: 0.8613354203078633, true: 1.0, loss: 0.14927127979264407\r",
      "predicted: 0.8615009537592659, true: 1.0, loss: 0.14907911592333672\r",
      "predicted: 0.8616661240268778, true: 1.0, loss: 0.14888741046395676\r",
      "predicted: 0.861830932226464, true: 1.0, loss: 0.14869616186333912\r",
      "predicted: 0.8619953794695142, true: 1.0, loss: 0.14850536857699317\r",
      "predicted: 0.8621594668632628, true: 1.0, loss: 0.14831502906706687\r",
      "predicted: 0.8623231955107067, true: 1.0, loss: 0.14812514180231418\r",
      "predicted: 0.8624865665106243, true: 1.0, loss: 0.14793570525806052\r",
      "predicted: 0.8626495809575944, true: 1.0, loss: 0.14774671791616925\r",
      "predicted: 0.8628122399420152, true: 1.0, loss: 0.14755817826500753\r",
      "predicted: 0.8629745445501222, true: 1.0, loss: 0.1473700847994137\r",
      "predicted: 0.8631364958640074, true: 1.0, loss: 0.14718243602066355\r",
      "predicted: 0.8632980949616369, true: 1.0, loss: 0.14699523043643795\r",
      "predicted: 0.8634593429168695, true: 1.0, loss: 0.14680846656078964\r",
      "predicted: 0.8636202407994753, true: 1.0, loss: 0.14662214291411088\r",
      "predicted: 0.8637807896751534, true: 1.0, loss: 0.14643625802310065\r",
      "predicted: 0.8639409906055493, true: 1.0, loss: 0.14625081042073376\r",
      "predicted: 0.8641008446482745, true: 1.0, loss: 0.1460657986462269\r",
      "predicted: 0.8642603528569227, true: 1.0, loss: 0.1458812212450086\r",
      "predicted: 0.8644195162810886, true: 1.0, loss: 0.14569707676868632\r",
      "predicted: 0.8645783359663848, true: 1.0, loss: 0.1455133637750161\r",
      "predicted: 0.86473681295446, true: 1.0, loss: 0.1453300808278705\r",
      "predicted: 0.8648949482830165, true: 1.0, loss: 0.1451472264972077\r",
      "predicted: 0.865052742985827, true: 1.0, loss: 0.14496479935904102\r",
      "predicted: 0.8652101980927525, true: 1.0, loss: 0.14478279799540789\r",
      "predicted: 0.8653673146297591, true: 1.0, loss: 0.14460122099433942\r",
      "predicted: 0.8655240936189361, true: 1.0, loss: 0.14442006694982965\r",
      "predicted: 0.8656805360785121, true: 1.0, loss: 0.14423933446180573\r",
      "predicted: 0.8658366430228721, true: 1.0, loss: 0.14405902213609847\r",
      "predicted: 0.8659924154625747, true: 1.0, loss: 0.1438791285844117\r",
      "predicted: 0.8661478544043693, true: 1.0, loss: 0.14369965242429245\r",
      "predicted: 0.866302960851212, true: 1.0, loss: 0.1435205922791024\r",
      "predicted: 0.8664577358022828, true: 1.0, loss: 0.14334194677798803\r",
      "predicted: 0.8666121802530022, true: 1.0, loss: 0.14316371455585147\r",
      "predicted: 0.8667662951950477, true: 1.0, loss: 0.14298589425332153\r",
      "predicted: 0.8669200816163698, true: 1.0, loss: 0.1428084845167251\r",
      "predicted: 0.867073540501209, true: 1.0, loss: 0.1426314839980583\r",
      "predicted: 0.8672266728301119, true: 1.0, loss: 0.14245489135495795\r",
      "predicted: 0.8673794795799467, true: 1.0, loss: 0.14227870525067401\r",
      "predicted: 0.8675319617239204, true: 1.0, loss: 0.1421029243540401\r",
      "predicted: 0.8676841202315941, true: 1.0, loss: 0.14192754733944696\r",
      "predicted: 0.8678359560688994, true: 1.0, loss: 0.14175257288681334\r",
      "predicted: 0.8679874701981535, true: 1.0, loss: 0.14157799968155974\r",
      "predicted: 0.8681386635780763, true: 1.0, loss: 0.14140382641457916\r",
      "predicted: 0.868289537163805, true: 1.0, loss: 0.1412300517822116\r",
      "predicted: 0.8684400919069101, true: 1.0, loss: 0.14105667448621556\r",
      "predicted: 0.8685903287554112, true: 1.0, loss: 0.1408836932337416\r",
      "predicted: 0.8687402486537923, true: 1.0, loss: 0.14071110673730547\r",
      "predicted: 0.8688898525430171, true: 1.0, loss: 0.14053891371476124\r",
      "predicted: 0.8690391413605448, true: 1.0, loss: 0.14036711288927456\r",
      "predicted: 0.8691881160403447, true: 1.0, loss: 0.14019570298929704\r",
      "predicted: 0.8693367775129124, true: 1.0, loss: 0.14002468274853863\r",
      "predicted: 0.8694851267052837, true: 1.0, loss: 0.13985405090594297\r",
      "predicted: 0.8696331645410507, true: 1.0, loss: 0.13968380620566043\r",
      "predicted: 0.8697808919403762, true: 1.0, loss: 0.1395139473970228\r",
      "predicted: 0.869928309820009, true: 1.0, loss: 0.1393444732345174\r",
      "predicted: 0.870075419093298, true: 1.0, loss: 0.13917538247776212\r",
      "predicted: 0.8702222206702083, true: 1.0, loss: 0.13900667389147867\r",
      "predicted: 0.8703687154573346, true: 1.0, loss: 0.13883834624546876\r",
      "predicted: 0.8705149043579162, true: 1.0, loss: 0.138670398314589\r",
      "predicted: 0.8706607882718521, true: 1.0, loss: 0.13850282887872475\r",
      "predicted: 0.8708063680957152, true: 1.0, loss: 0.13833563672276603\r",
      "predicted: 0.8709516447227658, true: 1.0, loss: 0.1381688206365838\r",
      "predicted: 0.8710966190429675, true: 1.0, loss: 0.13800237941500357\r",
      "predicted: 0.8712412919430004, true: 1.0, loss: 0.13783631185778275\r",
      "predicted: 0.8713856643062757, true: 1.0, loss: 0.13767061676958509\r",
      "predicted: 0.8715297370129502, true: 1.0, loss: 0.13750529295995717\r",
      "predicted: 0.8716735109399392, true: 1.0, loss: 0.13734033924330505\r",
      "predicted: 0.8718169869609316, true: 1.0, loss: 0.1371757544388693\r",
      "predicted: 0.8719601659464038, true: 1.0, loss: 0.13701153737070176\r",
      "predicted: 0.8721030487636329, true: 1.0, loss: 0.13684768686764245\r",
      "predicted: 0.8722456362767108, true: 1.0, loss: 0.13668420176329565\r",
      "predicted: 0.8723879293465586, true: 1.0, loss: 0.13652108089600626\r",
      "predicted: 0.872529928830939, true: 1.0, loss: 0.13635832310883775\r",
      "predicted: 0.8726716355844714, true: 1.0, loss: 0.13619592724954777\r",
      "predicted: 0.8728130504586437, true: 1.0, loss: 0.13603389217056658\r",
      "predicted: 0.8729541743018272, true: 1.0, loss: 0.1358722167289735\r",
      "predicted: 0.87309500795929, true: 1.0, loss: 0.13571089978647388\r",
      "predicted: 0.8732355522732088, true: 1.0, loss: 0.13554994020937802\r",
      "predicted: 0.8733758080826844, true: 1.0, loss: 0.135389336868577\r",
      "predicted: 0.8735157762237532, true: 1.0, loss: 0.13522908863952177\r",
      "predicted: 0.8736554575294009, true: 1.0, loss: 0.13506919440220055\r",
      "predicted: 0.8737948528295759, true: 1.0, loss: 0.1349096530411165\r",
      "predicted: 0.8739339629512018, true: 1.0, loss: 0.13475046344526656\r",
      "predicted: 0.874072788718191, true: 1.0, loss: 0.13459162450811887\r",
      "predicted: 0.874211330951457, true: 1.0, loss: 0.13443313512759175\r",
      "predicted: 0.8743495904689276, true: 1.0, loss: 0.13427499420603162\r",
      "predicted: 0.8744875680855573, true: 1.0, loss: 0.13411720065019245\r",
      "predicted: 0.8746252646133408, true: 1.0, loss: 0.1339597533712133\r",
      "predicted: 0.8747626808613246, true: 1.0, loss: 0.1338026512845985\r",
      "predicted: 0.8748998176356209, true: 1.0, loss: 0.13364589331019497\r",
      "predicted: 0.8750366757394187, true: 1.0, loss: 0.13348947837217295\r",
      "predicted: 0.8751732559729972, true: 1.0, loss: 0.13333340539900437\r",
      "predicted: 0.8753095591337384, true: 1.0, loss: 0.1331776733234417\r",
      "predicted: 0.8754455860161383, true: 1.0, loss: 0.13302228108249878\r",
      "predicted: 0.8755813374118209, true: 1.0, loss: 0.13286722761742836\r",
      "predicted: 0.8757168141095487, true: 1.0, loss: 0.13271251187370375\r",
      "predicted: 0.8758520168952361, true: 1.0, loss: 0.13255813280099746\r",
      "predicted: 0.8759869465519612, true: 1.0, loss: 0.1324040893531609\r",
      "predicted: 0.8761216038599775, true: 1.0, loss: 0.13225038048820525\r",
      "predicted: 0.8762559895967266, true: 1.0, loss: 0.1320970051682807\r",
      "predicted: 0.8763901045368494, true: 1.0, loss: 0.13194396235965722\r",
      "predicted: 0.8765239494521988, true: 1.0, loss: 0.13179125103270434\r",
      "predicted: 0.8766575251118506, true: 1.0, loss: 0.1316388701618724\r",
      "predicted: 0.8767908322821162, true: 1.0, loss: 0.13148681872567206\r",
      "predicted: 0.8769238717265535, true: 1.0, loss: 0.13133509570665597\r",
      "predicted: 0.8770566442059796, true: 1.0, loss: 0.1311837000913984\r",
      "predicted: 0.8771891504784813, true: 1.0, loss: 0.1310326308704771\r",
      "predicted: 0.8773213912994274, true: 1.0, loss: 0.13088188703845371\r",
      "predicted: 0.8774533674214801, true: 1.0, loss: 0.13073146759385484\r",
      "predicted: 0.8775850795946059, true: 1.0, loss: 0.13058137153915356\r",
      "predicted: 0.8777165285660878, true: 1.0, loss: 0.13043159788075026\r",
      "predicted: 0.8778477150805364, true: 1.0, loss: 0.13028214562895432\r",
      "predicted: 0.8779786398799007, true: 1.0, loss: 0.1301330137979654\r",
      "predicted: 0.8781093037034798, true: 1.0, loss: 0.12998420140585515\r",
      "predicted: 0.8782397072879344, true: 1.0, loss: 0.12983570747454862\r",
      "predicted: 0.8783698513672967, true: 1.0, loss: 0.1296875310298069\r",
      "predicted: 0.8784997366729832, true: 1.0, loss: 0.12953967110120745\r",
      "predicted: 0.8786293639338044, true: 1.0, loss: 0.12939212672212724\r",
      "predicted: 0.878758733875976, true: 1.0, loss: 0.12924489692972507\r",
      "predicted: 0.8788878472231303, true: 1.0, loss: 0.12909798076492274\r",
      "predicted: 0.8790167046963268, true: 1.0, loss: 0.12895137727238778\r",
      "predicted: 0.8791453070140628, true: 1.0, loss: 0.128805085500516\r",
      "predicted: 0.8792736548922844, true: 1.0, loss: 0.12865910450141396\r",
      "predicted: 0.8794017490443976, true: 1.0, loss: 0.12851343333088072\r",
      "predicted: 0.8795295901812781, true: 1.0, loss: 0.1283680710483918\r",
      "predicted: 0.8796571790112824, true: 1.0, loss: 0.128223016717081\r",
      "predicted: 0.8797845162402583, true: 1.0, loss: 0.12807826940372363\r",
      "predicted: 0.8799116025715559, true: 1.0, loss: 0.12793382817871907\r",
      "predicted: 0.880038438706037, true: 1.0, loss: 0.12778969211607413\r",
      "predicted: 0.8801650253420867, true: 1.0, loss: 0.1276458602933861\r",
      "predicted: 0.8802913631756226, true: 1.0, loss: 0.12750233179182618\r",
      "predicted: 0.8804174529001063, true: 1.0, loss: 0.127359105696122\r",
      "predicted: 0.8805432952065525, true: 1.0, loss: 0.1272161810945422\r",
      "predicted: 0.88066889078354, true: 1.0, loss: 0.12707355707887896\r",
      "predicted: 0.8807942403172222, true: 1.0, loss: 0.1269312327444316\r",
      "predicted: 0.8809193444913357, true: 1.0, loss: 0.12678920718999123\r",
      "predicted: 0.8810442039872124, true: 1.0, loss: 0.12664747951782285\r",
      "predicted: 0.8811688194837877, true: 1.0, loss: 0.12650604883365102\r",
      "predicted: 0.8812931916576117, true: 1.0, loss: 0.12636491424664237\r",
      "predicted: 0.8814173211828588, true: 1.0, loss: 0.12622407486939005\r",
      "predicted: 0.8815412087313368, true: 1.0, loss: 0.1260835298178987\r",
      "predicted: 0.8816648549724985, true: 1.0, loss: 0.12594327821156673\r",
      "predicted: 0.8817882605734495, true: 1.0, loss: 0.1258033191731726\r",
      "predicted: 0.8819114261989595, true: 1.0, loss: 0.1256636518288578\r",
      "predicted: 0.8820343525114711, true: 1.0, loss: 0.12552427530811178\r",
      "predicted: 0.8821570401711101, true: 1.0, loss: 0.12538518874375643\r",
      "predicted: 0.882279489835694, true: 1.0, loss: 0.12524639127193116\r",
      "predicted: 0.8824017021607435, true: 1.0, loss: 0.1251078820320762\r",
      "predicted: 0.8825236777994896, true: 1.0, loss: 0.1249696601669195\r",
      "predicted: 0.8826454174028853, true: 1.0, loss: 0.12483172482245922\r",
      "predicted: 0.8827669216196136, true: 1.0, loss: 0.1246940751479504\r",
      "predicted: 0.8828881910960973, true: 1.0, loss: 0.12455671029588948\r",
      "predicted: 0.8830092264765088, true: 1.0, loss: 0.12441962942199881\r",
      "predicted: 0.8831300284027789, true: 1.0, loss: 0.12428283168521252\r",
      "predicted: 0.8832505975146057, true: 1.0, loss: 0.1241463162476619\r",
      "predicted: 0.8833709344494647, true: 1.0, loss: 0.12401008227465994\r",
      "predicted: 0.8834910398426178, true: 1.0, loss: 0.123874128934687\r",
      "predicted: 0.8836109143271215, true: 1.0, loss: 0.12373845539937717\r",
      "predicted: 0.8837305585338371, true: 1.0, loss: 0.12360306084350256\r",
      "predicted: 0.8838499730914393, true: 1.0, loss: 0.12346794444495962\r",
      "predicted: 0.8839691586264254, true: 1.0, loss: 0.12333310538475453\r",
      "predicted: 0.8840881157631234, true: 1.0, loss: 0.12319854284698999\r",
      "predicted: 0.8842068451237025, true: 1.0, loss: 0.12306425601884907\r",
      "predicted: 0.8843253473281801, true: 1.0, loss: 0.12293024409058366\r",
      "predicted: 0.8844436229944326, true: 1.0, loss: 0.1227965062554981\r",
      "predicted: 0.8845616727382026, true: 1.0, loss: 0.12266304170993675\r",
      "predicted: 0.8846794971731081, true: 1.0, loss: 0.12252984965327035\r",
      "predicted: 0.8847970969106522, true: 1.0, loss: 0.12239692928788044\r",
      "predicted: 0.8849144725602298, true: 1.0, loss: 0.12226427981914832\r",
      "predicted: 0.8850316247291384, true: 1.0, loss: 0.12213190045543876\r",
      "predicted: 0.885148554022585, true: 1.0, loss: 0.1219997904080886\r",
      "predicted: 0.8852652610436956, true: 1.0, loss: 0.12186794889139173\r",
      "predicted: 0.8853817463935234, true: 1.0, loss: 0.12173637512258682\r",
      "predicted: 0.8854980106710574, true: 1.0, loss: 0.12160506832184306\r",
      "predicted: 0.8856140544732304, true: 1.0, loss: 0.12147402771224775\r",
      "predicted: 0.8857298783949281, true: 1.0, loss: 0.1213432525197923\r",
      "predicted: 0.8858454830289969, true: 1.0, loss: 0.12121274197335982\r",
      "predicted: 0.8859608689662524, true: 1.0, loss: 0.12108249530471159\r",
      "predicted: 0.8860760367954877, true: 1.0, loss: 0.12095251174847406\r",
      "predicted: 0.8861909871034817, true: 1.0, loss: 0.12082279054212648\r",
      "predicted: 0.8863057204750071, true: 1.0, loss: 0.12069333092598712\r",
      "predicted: 0.886420237492839, true: 1.0, loss: 0.12056413214320119\r",
      "predicted: 0.886534538737762, true: 1.0, loss: 0.12043519343972865\r",
      "predicted: 0.88664862478858, true: 1.0, loss: 0.12030651406432953\r",
      "predicted: 0.886762496222122, true: 1.0, loss: 0.12017809326855419\r",
      "predicted: 0.8868761536132528, true: 1.0, loss: 0.12004993030672777\r",
      "predicted: 0.8869895975348784, true: 1.0, loss: 0.11992202443594038\r",
      "predicted: 0.8871028285579555, true: 1.0, loss: 0.11979437491603316\r",
      "predicted: 0.8872158472514993, true: 1.0, loss: 0.11966698100958618\r",
      "predicted: 0.8873286541825903, true: 1.0, loss: 0.11953984198190686\r",
      "predicted: 0.8874412499163836, true: 1.0, loss: 0.11941295710101688\r",
      "predicted: 0.8875536350161158, true: 1.0, loss: 0.11928632563764038\r",
      "predicted: 0.8876658100431127, true: 1.0, loss: 0.11915994686519239\r",
      "predicted: 0.8877777755567974, true: 1.0, loss: 0.1190338200597662\r",
      "predicted: 0.8878895321146976, true: 1.0, loss: 0.11890794450012175\r",
      "predicted: 0.8880010802724541, true: 1.0, loss: 0.1187823194676731\r",
      "predicted: 0.8881124205838273, true: 1.0, loss: 0.11865694424647756\r",
      "predicted: 0.8882235536007056, true: 1.0, loss: 0.11853181812322346\r",
      "predicted: 0.8883344798731125, true: 1.0, loss: 0.11840694038721818\r",
      "predicted: 0.8884451999492142, true: 1.0, loss: 0.11828231033037728\r",
      "predicted: 0.8885557143753272, true: 1.0, loss: 0.11815792724721216\r",
      "predicted: 0.8886660236959257, true: 1.0, loss: 0.1180337904348188\r",
      "predicted: 0.8887761284536491, true: 1.0, loss: 0.11790989919286642\r",
      "predicted: 0.8888860291893089, true: 1.0, loss: 0.117786252823586\r",
      "predicted: 0.888995726441897, true: 1.0, loss: 0.1176628506317588\r",
      "predicted: 0.8891052207485918, true: 1.0, loss: 0.11753969192470537\r",
      "predicted: 0.8892145126447664, true: 1.0, loss: 0.11741677601227421\r",
      "predicted: 0.8893236026639957, true: 1.0, loss: 0.11729410220683015\r",
      "predicted: 0.8894324913380632, true: 1.0, loss: 0.11717166982324395\r",
      "predicted: 0.8895411791969684, true: 1.0, loss: 0.11704947817888116\r",
      "predicted: 0.889649666768934, true: 1.0, loss: 0.11692752659359056\r",
      "predicted: 0.8897579545804131, true: 1.0, loss: 0.11680581438969356\r",
      "predicted: 0.889866043156096, true: 1.0, loss: 0.11668434089197349\r",
      "predicted: 0.8899739330189176, true: 1.0, loss: 0.11656310542766436\r",
      "predicted: 0.8900816246900637, true: 1.0, loss: 0.11644210732644095\r",
      "predicted: 0.8901891186889793, true: 1.0, loss: 0.11632134592040644\r",
      "predicted: 0.890296415533374, true: 1.0, loss: 0.1162008205440838\r",
      "predicted: 0.89040351573923, true: 1.0, loss: 0.11608053053440359\r",
      "predicted: 0.8905104198208088, true: 1.0, loss: 0.11596047523069399\r",
      "predicted: 0.8906171282906574, true: 1.0, loss: 0.1158406539746706\r",
      "predicted: 0.890723641659616, true: 1.0, loss: 0.11572106611042547\r",
      "predicted: 0.8908299604368246, true: 1.0, loss: 0.11560171098441673\r",
      "predicted: 0.890936085129729, true: 1.0, loss: 0.11548258794545889\r",
      "predicted: 0.8910420162440884, true: 1.0, loss: 0.11536369634471191\r",
      "predicted: 0.8911477542839822, true: 1.0, loss: 0.11524503553567078\r",
      "predicted: 0.8912532997518154, true: 1.0, loss: 0.11512660487415649\r",
      "predicted: 0.891358653148327, true: 1.0, loss: 0.1150084037183045\r",
      "predicted: 0.8914638149725953, true: 1.0, loss: 0.11489043142855526\r",
      "predicted: 0.891568785722045, true: 1.0, loss: 0.11477268736764457\r",
      "predicted: 0.8916735658924536, true: 1.0, loss: 0.11465517090059305\r",
      "predicted: 0.8917781559779581, true: 1.0, loss: 0.1145378813946964\r",
      "predicted: 0.8918825564710611, true: 1.0, loss: 0.11442081821951564\r",
      "predicted: 0.891986767862638, true: 1.0, loss: 0.11430398074686682\r",
      "predicted: 0.8920907906419425, true: 1.0, loss: 0.11418736835081199\r",
      "predicted: 0.8921946252966138, true: 1.0, loss: 0.11407098040764896\r",
      "predicted: 0.8922982723126823, true: 1.0, loss: 0.1139548162959016\r",
      "predicted: 0.8924017321745771, true: 1.0, loss: 0.11383887539631005\r",
      "predicted: 0.8925050053651301, true: 1.0, loss: 0.11372315709182247\r",
      "predicted: 0.892608092365585, true: 1.0, loss: 0.11360766076758334\r",
      "predicted: 0.8927109936556019, true: 1.0, loss: 0.11349238581092541\r",
      "predicted: 0.8928137097132636, true: 1.0, loss: 0.11337733161136025\r",
      "predicted: 0.892916241015082, true: 1.0, loss: 0.1132624975605685\r",
      "predicted: 0.8930185880360052, true: 1.0, loss: 0.11314788305239008\r",
      "predicted: 0.8931207512494218, true: 1.0, loss: 0.11303348748281611\r",
      "predicted: 0.8932227311271688, true: 1.0, loss: 0.11291931024997837\r",
      "predicted: 0.8933245281395362, true: 1.0, loss: 0.11280535075414123\r",
      "predicted: 0.8934261427552747, true: 1.0, loss: 0.11269160839769136\r",
      "predicted: 0.8935275754416001, true: 1.0, loss: 0.11257808258512947\r",
      "predicted: 0.8936288266642003, true: 1.0, loss: 0.11246477272306121\r",
      "predicted: 0.8937298968872412, true: 1.0, loss: 0.11235167822018731\r",
      "predicted: 0.8938307865733722, true: 1.0, loss: 0.11223879848729591\r",
      "predicted: 0.8939314961837326, true: 1.0, loss: 0.11212613293725246\r",
      "predicted: 0.8940320261779574, true: 1.0, loss: 0.11201368098499118\r",
      "predicted: 0.8941323770141828, true: 1.0, loss: 0.1119014420475069\r",
      "predicted: 0.8942325491490526, true: 1.0, loss: 0.1117894155438453\r",
      "predicted: 0.8943325430377238, true: 1.0, loss: 0.11167760089509451\r",
      "predicted: 0.8944323591338723, true: 1.0, loss: 0.11156599752437661\r",
      "predicted: 0.8945319978896986, true: 1.0, loss: 0.11145460485683895\r",
      "predicted: 0.8946314597559338, true: 1.0, loss: 0.11134342231964521\r",
      "predicted: 0.8947307451818458, true: 1.0, loss: 0.11123244934196648\r",
      "predicted: 0.8948298546152434, true: 1.0, loss: 0.11112168535497413\r",
      "predicted: 0.8949287885024836, true: 1.0, loss: 0.11101112979183009\r",
      "predicted: 0.8950275472884767, true: 1.0, loss: 0.11090078208767823\r",
      "predicted: 0.8951261314166913, true: 1.0, loss: 0.11079064167963731\r",
      "predicted: 0.8952245413291613, true: 1.0, loss: 0.11068070800679077\r",
      "predicted: 0.8953227774664897, true: 1.0, loss: 0.11057098051018034\r",
      "predicted: 0.8954208402678558, true: 1.0, loss: 0.11046145863279598\r",
      "predicted: 0.8955187301710199, true: 1.0, loss: 0.11035214181956865\r",
      "predicted: 0.8956164476123283, true: 1.0, loss: 0.11024302951736234\r",
      "predicted: 0.89571399302672, true: 1.0, loss: 0.11013412117496486\r",
      "predicted: 0.8958113668477311, true: 1.0, loss: 0.11002541624308058\r",
      "predicted: 0.8959085695075009, true: 1.0, loss: 0.10991691417432217\r",
      "predicted: 0.8960056014367768, true: 1.0, loss: 0.10980861442320228\r",
      "predicted: 0.8961024630649204, true: 1.0, loss: 0.10970051644612537\r",
      "predicted: 0.8961991548199114, true: 1.0, loss: 0.10959261970138108\r",
      "predicted: 0.896295677128355, true: 1.0, loss: 0.10948492364913419\r",
      "predicted: 0.8963920304154852, true: 1.0, loss: 0.10937742775141869\r",
      "predicted: 0.8964882151051716, true: 1.0, loss: 0.10927013147212843\r",
      "predicted: 0.8965842316199234, true: 1.0, loss: 0.10916303427701071\r",
      "predicted: 0.8966800803808959, true: 1.0, loss: 0.10905613563365703\r",
      "predicted: 0.8967757618078948, true: 1.0, loss: 0.1089494350114963\r",
      "predicted: 0.8968712763193813, true: 1.0, loss: 0.10884293188178734\r",
      "predicted: 0.8969666243324784, true: 1.0, loss: 0.10873662571761028\r",
      "predicted: 0.8970618062629746, true: 1.0, loss: 0.10863051599385981\r",
      "predicted: 0.8971568225253299, true: 1.0, loss: 0.10852460218723714\r",
      "predicted: 0.8972516735326808, true: 1.0, loss: 0.10841888377624259\r",
      "predicted: 0.8973463596968451, true: 1.0, loss: 0.10831336024116797\r",
      "predicted: 0.8974408814283273, true: 1.0, loss: 0.10820803106408913\r",
      "predicted: 0.8975352391363233, true: 1.0, loss: 0.10810289572885898\r",
      "predicted: 0.8976294332287253, true: 1.0, loss: 0.1079979537210994\r",
      "predicted: 0.8977234641121276, true: 1.0, loss: 0.10789320452819426\r",
      "predicted: 0.8978173321918307, true: 1.0, loss: 0.1077886476392816\r",
      "predicted: 0.8979110378718465, true: 1.0, loss: 0.10768428254524737\r",
      "predicted: 0.8980045815549031, true: 1.0, loss: 0.10758010873871725\r",
      "predicted: 0.8980979636424503, true: 1.0, loss: 0.10747612571404974\r",
      "predicted: 0.898191184534664, true: 1.0, loss: 0.10737233296732862\r",
      "predicted: 0.8982842446304503, true: 1.0, loss: 0.10726872999635689\r",
      "predicted: 0.8983771443274523, true: 1.0, loss: 0.10716531630064803\r",
      "predicted: 0.8984698840220529, true: 1.0, loss: 0.1070620913814204\r",
      "predicted: 0.8985624641093806, true: 1.0, loss: 0.1069590547415893\r",
      "predicted: 0.8986548849833146, true: 1.0, loss: 0.10685620588575988\r",
      "predicted: 0.8987471470364882, true: 1.0, loss: 0.10675354432022138\r",
      "predicted: 0.8988392506602952, true: 1.0, loss: 0.10665106955293822\r",
      "predicted: 0.8989311962448933, true: 1.0, loss: 0.10654878109354513\r",
      "predicted: 0.8990229841792093, true: 1.0, loss: 0.10644667845333862\r",
      "predicted: 0.8991146148509442, true: 1.0, loss: 0.10634476114527093\r",
      "predicted: 0.8992060886465769, true: 1.0, loss: 0.10624302868394336\r",
      "predicted: 0.8992974059513693, true: 1.0, loss: 0.1061414805855992\r",
      "predicted: 0.8993885671493715, true: 1.0, loss: 0.10604011636811644\r",
      "predicted: 0.8994795726234254, true: 1.0, loss: 0.10593893555100224\r",
      "predicted: 0.8995704227551692, true: 1.0, loss: 0.10583793765538575"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "predicted: 0.8996611179250436, true: 1.0, loss: 0.10573712220401045\r",
      "predicted: 0.8997516585122943, true: 1.0, loss: 0.10563648872122897\r",
      "predicted: 0.8998420448949777, true: 1.0, loss: 0.10553603673299607\r",
      "predicted: 0.8999322774499648, true: 1.0, loss: 0.10543576576686157\r",
      "predicted: 0.9000223565529463, true: 1.0, loss: 0.10533567535196452\r",
      "predicted: 0.9001122825784361, true: 1.0, loss: 0.10523576501902625\r",
      "predicted: 0.9002020558997771, true: 1.0, loss: 0.1051360343003438\r",
      "predicted: 0.900291676889144, true: 1.0, loss: 0.10503648272978418\r",
      "predicted: 0.9003811459175491, true: 1.0, loss: 0.1049371098427771\r",
      "predicted: 0.9004704633548456, true: 1.0, loss: 0.10483791517630951\r",
      "predicted: 0.900559629569733, true: 1.0, loss: 0.10473889826891795\r",
      "predicted: 0.9006486449297605, true: 1.0, loss: 0.10464005866068365\r",
      "predicted: 0.9007375098013316, true: 1.0, loss: 0.1045413958932256\r",
      "predicted: 0.9008262245497086, true: 1.0, loss: 0.10444290950969426\r",
      "predicted: 0.9009147895390169, true: 1.0, loss: 0.10434459905476526\r",
      "predicted: 0.9010032051322491, true: 1.0, loss: 0.10424646407463346\r",
      "predicted: 0.9010914716912688, true: 1.0, loss: 0.10414850411700695\r",
      "predicted: 0.9011795895768161, true: 1.0, loss: 0.10405071873109999\r",
      "predicted: 0.90126755914851, true: 1.0, loss: 0.10395310746762862\r",
      "predicted: 0.9013553807648547, true: 1.0, loss: 0.10385566987880221\r",
      "predicted: 0.9014430547832414, true: 1.0, loss: 0.10375840551832012\r",
      "predicted: 0.9015305815599548, true: 1.0, loss: 0.10366131394136319\r",
      "predicted: 0.9016179614501755, true: 1.0, loss: 0.10356439470458949\r",
      "predicted: 0.9017051948079849, true: 1.0, loss: 0.1034676473661274\r",
      "predicted: 0.9017922819863693, true: 1.0, loss: 0.10337107148557022\r",
      "predicted: 0.9018792233372234, true: 1.0, loss: 0.10327466662397009\r",
      "predicted: 0.9019660192113554, true: 1.0, loss: 0.10317843234383148\r",
      "predicted: 0.9020526699584899, true: 1.0, loss: 0.10308236820910686\r",
      "predicted: 0.9021391759272724, true: 1.0, loss: 0.10298647378518945\r",
      "predicted: 0.9022255374652741, true: 1.0, loss: 0.10289074863890763\r",
      "predicted: 0.9023117549189942, true: 1.0, loss: 0.10279519233852023\r",
      "predicted: 0.9023978286338658, true: 1.0, loss: 0.10269980445370899\r",
      "predicted: 0.9024837589542577, true: 1.0, loss: 0.10260458455557518\r",
      "predicted: 0.902569546223481, true: 1.0, loss: 0.1025095322166312\r",
      "predicted: 0.9026551907837902, true: 1.0, loss: 0.10241464701079753\r",
      "predicted: 0.9027406929763894, true: 1.0, loss: 0.10231992851339515\r",
      "predicted: 0.9028260531414347, true: 1.0, loss: 0.10222537630114105\r",
      "predicted: 0.9029112716180392, true: 1.0, loss: 0.10213098995214193\r",
      "predicted: 0.9029963487442754, true: 1.0, loss: 0.10203676904588976\r",
      "predicted: 0.9030812848571812, true: 1.0, loss: 0.10194271316325411\r",
      "predicted: 0.9031660802927611, true: 1.0, loss: 0.10184882188647963\r",
      "predicted: 0.9032507353859923, true: 1.0, loss: 0.10175509479917796\r",
      "predicted: 0.9033352504708273, true: 1.0, loss: 0.10166153148632347\r",
      "predicted: 0.903419625880198, true: 1.0, loss: 0.10156813153424761\r",
      "predicted: 0.9035038619460193, true: 1.0, loss: 0.10147489453063362\r",
      "predicted: 0.9035879589991929, true: 1.0, loss: 0.10138182006451124\r",
      "predicted: 0.9036719173696109, true: 1.0, loss: 0.10128890772625121\r",
      "predicted: 0.9037557373861604, true: 1.0, loss: 0.10119615710755946\r",
      "predicted: 0.903839419376726, true: 1.0, loss: 0.10110356780147263\r",
      "predicted: 0.9039229636681936, true: 1.0, loss: 0.10101113940235262\r",
      "predicted: 0.9040063705864553, true: 1.0, loss: 0.10091887150588061\r",
      "predicted: 0.9040896404564112, true: 1.0, loss: 0.10082676370905325\r",
      "predicted: 0.9041727736019748, true: 1.0, loss: 0.1007348156101757\r",
      "predicted: 0.9042557703460756, true: 1.0, loss: 0.1006430268088577\r",
      "predicted: 0.9043386310106627, true: 1.0, loss: 0.10055139690600841\r",
      "predicted: 0.9044213559167091, true: 1.0, loss: 0.10045992550383012\r",
      "predicted: 0.9045039453842145, true: 1.0, loss: 0.10036861220581457\r",
      "predicted: 0.9045863997322089, true: 1.0, loss: 0.1002774566167374\r",
      "predicted: 0.904668719278757, true: 1.0, loss: 0.10018645834265216\r",
      "predicted: 0.9047509043409607, true: 1.0, loss: 0.10009561699088677\r",
      "predicted: 0.9048329552349631, true: 1.0, loss: 0.1000049321700376\r",
      "predicted: 0.9049148722759522, true: 1.0, loss: 0.09991440348996425\r",
      "predicted: 0.9049966557781639, true: 1.0, loss: 0.09982403056178553\r",
      "predicted: 0.9050783060548856, true: 1.0, loss: 0.09973381299787383\r",
      "predicted: 0.9051598234184601, true: 1.0, loss: 0.09964375041185003\r",
      "predicted: 0.9052412081802882, true: 1.0, loss: 0.09955384241857959\r",
      "predicted: 0.9053224606508333, true: 1.0, loss: 0.09946408863416599\r",
      "predicted: 0.9054035811396236, true: 1.0, loss: 0.09937448867594775\r",
      "predicted: 0.9054845699552558, true: 1.0, loss: 0.09928504216249273\r",
      "predicted: 0.9055654274053997, true: 1.0, loss: 0.09919574871359246\r",
      "predicted: 0.9056461537967994, true: 1.0, loss: 0.09910660795025929\r",
      "predicted: 0.9057267494352786, true: 1.0, loss: 0.09901761949471967\r",
      "predicted: 0.9058072146257429, true: 1.0, loss: 0.098928782970411\r",
      "predicted: 0.9058875496721835, true: 1.0, loss: 0.09884009800197542\r",
      "predicted: 0.9059677548776804, true: 1.0, loss: 0.0987515642152565\r",
      "predicted: 0.9060478305444054, true: 1.0, loss: 0.09866318123729392\r",
      "predicted: 0.9061277769736263, true: 1.0, loss: 0.09857494869631818\r",
      "predicted: 0.9062075944657089, true: 1.0, loss: 0.09848686622174754\r",
      "predicted: 0.9062872833201212, true: 1.0, loss: 0.09839893344418181\r",
      "predicted: 0.9063668438354368, true: 1.0, loss: 0.09831114999539836\r",
      "predicted: 0.9064462763093367, true: 1.0, loss: 0.09822351550834825\r",
      "predicted: 0.9065255810386146, true: 1.0, loss: 0.09813602961715029\r",
      "predicted: 0.9066047583191781, true: 1.0, loss: 0.09804869195708789\r",
      "predicted: 0.9066838084460533, true: 1.0, loss: 0.09796150216460345\r",
      "predicted: 0.9067627317133876, true: 1.0, loss: 0.09787445987729437\r",
      "predicted: 0.9068415284144521, true: 1.0, loss: 0.09778756473390901\r",
      "predicted: 0.906920198841646, true: 1.0, loss: 0.09770081637434114\r",
      "predicted: 0.9069987432864988, true: 1.0, loss: 0.09761421443962655\r",
      "predicted: 0.9070771620396739, true: 1.0, loss: 0.09752775857193792\r",
      "predicted: 0.9071554553909718, true: 1.0, loss: 0.09744144841458079\r",
      "predicted: 0.9072336236293319, true: 1.0, loss: 0.0973552836119898\r",
      "predicted: 0.907311667042838, true: 1.0, loss: 0.09726926380972258\r",
      "predicted: 0.9073895859187197, true: 1.0, loss: 0.09718338865445686\r",
      "predicted: 0.9074673805433546, true: 1.0, loss: 0.0970976577939866\r",
      "predicted: 0.907545051202274, true: 1.0, loss: 0.09701207087721589\r",
      "predicted: 0.9076225981801634, true: 1.0, loss: 0.09692662755415647\r",
      "predicted: 0.9077000217608677, true: 1.0, loss: 0.09684132747592163\r",
      "predicted: 0.9077773222273917, true: 1.0, loss: 0.09675617029472443\r",
      "predicted: 0.9078544998619056, true: 1.0, loss: 0.09667115566387098\r",
      "predicted: 0.9079315549457461, true: 1.0, loss: 0.09658628323775809\r",
      "predicted: 0.9080084877594208, true: 1.0, loss: 0.09650155267186766\r",
      "predicted: 0.9080852985826098, true: 1.0, loss: 0.09641696362276408\r",
      "predicted: 0.90816198769417, true: 1.0, loss: 0.09633251574808838\r",
      "predicted: 0.9082385553721366, true: 1.0, loss: 0.0962482087065559\r",
      "predicted: 0.9083150018937276, true: 1.0, loss: 0.09616404215795032\r",
      "predicted: 0.9083913275353452, true: 1.0, loss: 0.09608001576312124\r",
      "predicted: 0.9084675325725803, true: 1.0, loss: 0.0959961291839786\r",
      "predicted: 0.9085436172802134, true: 1.0, loss: 0.09591238208349027\r",
      "predicted: 0.9086195819322193, true: 1.0, loss: 0.09582877412567663\r",
      "predicted: 0.9086954268017688, true: 1.0, loss: 0.0957453049756073\r",
      "predicted: 0.9087711521612327, true: 1.0, loss: 0.09566197429939657\r",
      "predicted: 0.9088467582821826, true: 1.0, loss: 0.09557878176420079\r",
      "predicted: 0.9089222454353966, true: 1.0, loss: 0.09549572703821202\r",
      "predicted: 0.9089976138908594, true: 1.0, loss: 0.09541280979065637\r",
      "predicted: 0.9090728639177668, true: 1.0, loss: 0.09533002969178914"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    model.step(x, y, loss, optim=optim)\n",
    "    y_pred = model(x)\n",
    "    print(\"\\r\"+f\"predicted: {y_pred[0][0]}, true: {y[0][0]}, loss: {loss(y_pred=y_pred, y_true=y)[0][0]}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
